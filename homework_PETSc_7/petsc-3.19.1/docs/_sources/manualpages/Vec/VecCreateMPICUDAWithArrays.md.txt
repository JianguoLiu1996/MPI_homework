:orphan:
# VecCreateMPICUDAWithArrays
Creates a parallel, array-style vector using CUDA, where the user provides the complete array space to store the vector values. 
## Synopsis
```
#include <petscvec.h> 
PetscErrorCode VecCreateMPICUDAWithArrays(MPI_Comm comm, PetscInt bs, PetscInt n, PetscInt N, const PetscScalar cpuarray[], const PetscScalar gpuarray[], Vec *v)
```
Collective, Possibly Synchronous


## Input Parameters

- ***comm     -*** the MPI communicator to use
- ***bs       -*** block size, same meaning as `VecSetBlockSize()`
- ***n        -*** local vector length, cannot be `PETSC_DECIDE`
- ***N        -*** global vector length (or `PETSC_DECIDE` to have calculated)
- ***cpuarray -*** CPU memory where the vector elements are to be stored (or `NULL`)
- ***gpuarray -*** GPU memory where the vector elements are to be stored (or `NULL`)



## Output Parameter

- ***v -*** the vector



## Notes
See `VecCreateSeqCUDAWithArrays()` for further discussion, this routine shares identical
semantics.




## See Also
 `VecCreateMPICUDA()`, `VecCreateSeqCUDAWithArrays()`, `VecCreateMPIWithArray()`,
`VecCreateSeqWithArray()`, `VecCreate()`, `VecDuplicate()`, `VecDuplicateVecs()`, `VecCreateGhost()`,
`VecCreateMPI()`, `VecCreateGhostWithArray()`, `VecPlaceArray()`

## Level
intermediate

## Location
<A HREF="PETSC_DOC_OUT_ROOT_PLACEHOLDER/src/vec/vec/impls/mpi/cupm/cuda/vecmpicupm.cu.html#VecCreateMPICUDAWithArrays">src/vec/vec/impls/mpi/cupm/cuda/vecmpicupm.cu</A>


---
[Edit on GitLab](https://gitlab.com/petsc/petsc/-/edit/release/src/vec/vec/impls/mpi/cupm/cuda/vecmpicupm.cu)


[Index of all Vec routines](index.md)  
[Table of Contents for all manual pages](/manualpages/index.md)  
[Index of all manual pages](/manualpages/singleindex.md)  
