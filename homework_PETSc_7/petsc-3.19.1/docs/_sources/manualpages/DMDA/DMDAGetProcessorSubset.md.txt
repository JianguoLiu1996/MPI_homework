:orphan:
# DMDAGetProcessorSubset
Returns a communicator consisting only of the processors in a `DMDA` that own a particular global x, y, or z grid point (corresponding to a logical plane in a 3D grid or a line in a 2D grid). 
## Synopsis
```
#include "petscdmda.h"   
PetscErrorCode DMDAGetProcessorSubset(DM da, DMDirection dir, PetscInt gp, MPI_Comm *comm)
```
Collective; No Fortran Support


## Input Parameters

- ***da -*** the distributed array
- ***dir -*** Cartesian direction, either `DM_X`, `DM_Y`, or `DM_Z`
- ***gp -*** global grid point number in this direction



## Output Parameter

- ***comm -*** new communicator





## Notes
All processors that share the `DMDA` must call this with the same gp value

After use, comm should be freed with `MPI_Comm_free()`

This routine is particularly useful to compute boundary conditions
or other application-specific calculations that require manipulating
sets of data throughout a logical plane of grid points.


## See Also
 `DM`, `DMDA`, `DMDirection`

## Level
advanced

## Location
<A HREF="PETSC_DOC_OUT_ROOT_PLACEHOLDER/src/dm/impls/da/dasub.c.html#DMDAGetProcessorSubset">src/dm/impls/da/dasub.c</A>

## Examples
<A HREF="PETSC_DOC_OUT_ROOT_PLACEHOLDER/src/dm/tutorials/ex22.c.html">src/dm/tutorials/ex22.c</A><BR>


---
[Edit on GitLab](https://gitlab.com/petsc/petsc/-/edit/release/src/dm/impls/da/dasub.c)


[Index of all DMDA routines](index.md)  
[Table of Contents for all manual pages](/manualpages/index.md)  
[Index of all manual pages](/manualpages/singleindex.md)  
