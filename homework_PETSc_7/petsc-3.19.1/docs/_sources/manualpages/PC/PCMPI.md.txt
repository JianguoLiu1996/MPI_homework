:orphan:
# PCMPI
Calls an MPI parallel `KSP` to solve a linear system from user code running on one process 
## Options Database Keys

- ***-mpi_linear_solver_server -*** causes the PETSc program to start in MPI linear solver server mode where only the first MPI rank runs user code
- ***-mpi_linear_solver_server_view -*** displays information about the linear systems solved by the MPI linear solver server
- ***-pc_mpi_minimum_count_per_rank -*** sets the minimum size of the linear system per MPI rank that the solver will strive for
- ***-pc_mpi_always_use_server -*** use the server solver code even if the particular system is only solved on the process, this option is only for debugging and testing purposes





## Notes
The options database prefix for the MPI parallel `KSP` and `PC` is -mpi_

The MPI linear solver server will not support scaling user code to utilize extremely large numbers of MPI ranks but should give reasonable speedup for
potentially 4 to 8 MPI ranks depending on the linear system being solved, solver algorithm, and the hardware.

It can be particularly useful for user OpenMP code or potentially user GPU code.

When the program is running with a single MPI rank then this directly uses the provided matrix and right hand side (still in a `KSP` with the options prefix of -mpi)
and does not need to distribute the matrix and vector to the various MPI ranks; thus it incurs no extra overhead over just using the `KSP` directly.

The solver options for `KSP` and `PC` must be controlled via the options database, calls to set options directly on the user level `KSP` and `PC` have no effect
because they are not the actual solver objects.

When `-log_view` is used with this solver the events within the parallel solve are logging in their own stage. Some of the logging in the other
stages will be confusing since the event times are only recorded on the 0th MPI rank, thus the percent of time in the events will be misleading.


## See Also
 `KSPCreate()`, `KSPSetType()`, `KSPType`, `KSP`, `PC`, `PCMPIServerBegin()`, `PCMPIServerEnd()`

## Level
beginner

## Location
<A HREF="PETSC_DOC_OUT_ROOT_PLACEHOLDER/src/ksp/pc/impls/mpi/pcmpi.c.html#PCMPI">src/ksp/pc/impls/mpi/pcmpi.c</A>


---
[Edit on GitLab](https://gitlab.com/petsc/petsc/-/edit/release/src/ksp/pc/impls/mpi/pcmpi.c)


[Index of all PC routines](index.md)  
[Table of Contents for all manual pages](/manualpages/index.md)  
[Index of all manual pages](/manualpages/singleindex.md)  
