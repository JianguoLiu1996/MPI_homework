<center><a href="https://gitlab.com/petsc/petsc/-/blob/a093114c2e7b4ba1f47e6ac8a72c57724150f16d/src/vec/is/sf/impls/basic/sfmpi.c">Actual source code: sfmpi.c</a></center><br>

<html>
<head>
<title></title>
<meta name="generator" content="c2html 0.9.4">
<meta name="date" content="2023-04-30T14:19:21+00:00">
</head>

<body bgcolor="#FFFFFF">
<pre width="80">
<a name="line1">  1: </a><font color="#B22222">/* Mainly for <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Isend.html#MPI_Isend">MPI_Isend</a> in SFBASIC. Once SFNEIGHBOR, SFALLGHATERV etc have a persistent version,</font>
<a name="line2">  2: </a><font color="#B22222">   we can also do abstractions like Prepare/StartCommunication.</font>
<a name="line3">  3: </a><font color="#B22222">*/</font>

<a name="line5">  5: </a>#include <A href="sfpack.h.html">&lt;../src/vec/is/sf/impls/basic/sfpack.h&gt;</A>

<a name="line7">  7: </a><font color="#B22222">/* Start MPI requests. If use non-GPU aware MPI, we might need to copy data from device buf to host buf */</font>
<a name="line8">  8: </a><strong><font color="#4169E1"><a name="PetscSFLinkStartRequests_MPI"></a>static <a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFLinkStartRequests_MPI(<a href="../../../../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> sf, PetscSFLink link, PetscSFDirection direction)</font></strong>
<a name="line9">  9: </a>{
<a name="line10"> 10: </a>  <a href="../../../../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a>    nreqs;
<a name="line11"> 11: </a>  MPI_Request   *reqs = NULL;
<a name="line12"> 12: </a>  PetscSF_Basic *bas  = (PetscSF_Basic *)sf-&gt;data;
<a name="line13"> 13: </a>  <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>       buflen;

<a name="line15"> 15: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionBegin.html">PetscFunctionBegin</a>;
<a name="line16"> 16: </a>  buflen = (direction == PETSCSF_../../../../../..2LEAF) ? sf-&gt;leafbuflen[PETSCSF_REMOTE] : bas-&gt;rootbuflen[PETSCSF_REMOTE];
<a name="line17"> 17: </a>  <font color="#4169E1">if</font> (buflen) {
<a name="line18"> 18: </a>    <font color="#4169E1">if</font> (direction == PETSCSF_../../../../../..2LEAF) {
<a name="line19"> 19: </a>      nreqs = sf-&gt;nleafreqs;
<a name="line20"> 20: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkGetMPIBuffersAndRequests(sf, link, direction, NULL, NULL, NULL, &amp;reqs));
<a name="line21"> 21: </a>    } <font color="#4169E1">else</font> { <font color="#B22222">/* leaf to root */</font>
<a name="line22"> 22: </a>      nreqs = bas-&gt;nrootreqs;
<a name="line23"> 23: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkGetMPIBuffersAndRequests(sf, link, direction, NULL, NULL, &amp;reqs, NULL));
<a name="line24"> 24: </a>    }
<a name="line25"> 25: </a>    <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPI_Startall_irecv(buflen, link-&gt;unit, nreqs, reqs));
<a name="line26"> 26: </a>  }

<a name="line28"> 28: </a>  buflen = (direction == PETSCSF_../../../../../..2LEAF) ? bas-&gt;rootbuflen[PETSCSF_REMOTE] : sf-&gt;leafbuflen[PETSCSF_REMOTE];
<a name="line29"> 29: </a>  <font color="#4169E1">if</font> (buflen) {
<a name="line30"> 30: </a>    <font color="#4169E1">if</font> (direction == PETSCSF_../../../../../..2LEAF) {
<a name="line31"> 31: </a>      nreqs = bas-&gt;nrootreqs;
<a name="line32"> 32: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkCopyRootBufferInCaseNotUseGpuAwareMPI(sf, link, <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_TRUE</a> <font color="#B22222">/*device2host before sending */</font>));
<a name="line33"> 33: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkGetMPIBuffersAndRequests(sf, link, direction, NULL, NULL, &amp;reqs, NULL));
<a name="line34"> 34: </a>    } <font color="#4169E1">else</font> { <font color="#B22222">/* leaf to root */</font>
<a name="line35"> 35: </a>      nreqs = sf-&gt;nleafreqs;
<a name="line36"> 36: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkCopyLeafBufferInCaseNotUseGpuAwareMPI(sf, link, <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_TRUE</a>));
<a name="line37"> 37: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkGetMPIBuffersAndRequests(sf, link, direction, NULL, NULL, NULL, &amp;reqs));
<a name="line38"> 38: </a>    }
<a name="line39"> 39: </a>    <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkSyncStreamBeforeCallMPI(sf, link, direction));
<a name="line40"> 40: </a>    <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPI_Startall_isend(buflen, link-&gt;unit, nreqs, reqs));
<a name="line41"> 41: </a>  }
<a name="line42"> 42: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionReturn.html">PetscFunctionReturn</a>(<a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>);
<a name="line43"> 43: </a>}

<a name="line45"> 45: </a><strong><font color="#4169E1"><a name="PetscSFLinkWaitRequests_MPI"></a>static <a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFLinkWaitRequests_MPI(<a href="../../../../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> sf, PetscSFLink link, PetscSFDirection direction)</font></strong>
<a name="line46"> 46: </a>{
<a name="line47"> 47: </a>  PetscSF_Basic     *bas           = (PetscSF_Basic *)sf-&gt;data;
<a name="line48"> 48: </a>  const <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a> rootmtype_mpi = link-&gt;rootmtype_mpi, leafmtype_mpi = link-&gt;leafmtype_mpi;
<a name="line49"> 49: </a>  const <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>     rootdirect_mpi = link-&gt;rootdirect_mpi, leafdirect_mpi = link-&gt;leafdirect_mpi;

<a name="line51"> 51: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionBegin.html">PetscFunctionBegin</a>;
<a name="line52"> 52: </a>  <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(bas-&gt;nrootreqs, link-&gt;rootreqs[direction][rootmtype_mpi][rootdirect_mpi], MPI_STATUSES_IGNORE));
<a name="line53"> 53: </a>  <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(sf-&gt;nleafreqs, link-&gt;leafreqs[direction][leafmtype_mpi][leafdirect_mpi], MPI_STATUSES_IGNORE));
<a name="line54"> 54: </a>  <font color="#4169E1">if</font> (direction == PETSCSF_../../../../../..2LEAF) {
<a name="line55"> 55: </a>    <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkCopyLeafBufferInCaseNotUseGpuAwareMPI(sf, link, <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a> <font color="#B22222">/* host2device after recving */</font>));
<a name="line56"> 56: </a>  } <font color="#4169E1">else</font> {
<a name="line57"> 57: </a>    <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkCopyRootBufferInCaseNotUseGpuAwareMPI(sf, link, <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>));
<a name="line58"> 58: </a>  }
<a name="line59"> 59: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionReturn.html">PetscFunctionReturn</a>(<a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>);
<a name="line60"> 60: </a>}

<a name="line62"> 62: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPIX_STREAM)</font>
<a name="line63"> 63: </a>// issue MPIX_Isend/Irecv_enqueue()
<a name="line64"> 64: </a><strong><font color="#4169E1"><a name="PetscSFLinkStartEnqueue_MPIX_Stream"></a>static <a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFLinkStartEnqueue_MPIX_Stream(<a href="../../../../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> sf, PetscSFLink link, PetscSFDirection direction)</font></strong>
<a name="line65"> 65: </a>{
<a name="line66"> 66: </a>  PetscSF_Basic     *bas = (PetscSF_Basic *)sf-&gt;data;
<a name="line67"> 67: </a>  <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>           i, j, cnt, nrootranks, ndrootranks, nleafranks, ndleafranks;
<a name="line68"> 68: </a>  const <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *rootoffset, *leafoffset;
<a name="line69"> 69: </a>  MPI_Aint           disp;
<a name="line70"> 70: </a>  <a href="../../../../../../manualpages/Sys/MPI_Comm.html">MPI_Comm</a>           stream_comm   = sf-&gt;stream_comm;
<a name="line71"> 71: </a>  MPI_Datatype       unit          = link-&gt;unit;
<a name="line72"> 72: </a>  const <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a> rootmtype_mpi = link-&gt;rootmtype_mpi, leafmtype_mpi = link-&gt;leafmtype_mpi; <font color="#B22222">/* Used to select buffers passed to MPI */</font>
<a name="line73"> 73: </a>  const <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>     rootdirect_mpi = link-&gt;rootdirect_mpi, leafdirect_mpi = link-&gt;leafdirect_mpi;

<a name="line75"> 75: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionBegin.html">PetscFunctionBegin</a>;
<a name="line76"> 76: </a>  <font color="#4169E1">if</font> (bas-&gt;rootbuflen[PETSCSF_REMOTE]) {
<a name="line77"> 77: </a>    <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFGetRootInfo_Basic(sf, &amp;nrootranks, &amp;ndrootranks, NULL, &amp;rootoffset, NULL));
<a name="line78"> 78: </a>    <font color="#4169E1">if</font> (direction == PETSCSF_LEAF2../../../../../..) {
<a name="line79"> 79: </a>      <font color="#4169E1">for</font> (i = ndrootranks, j = 0; i &lt; nrootranks; i++, j++) {
<a name="line80"> 80: </a>        disp = (rootoffset[i] - rootoffset[ndrootranks]) * link-&gt;unitbytes;
<a name="line81"> 81: </a>        cnt  = rootoffset[i + 1] - rootoffset[i];
<a name="line82"> 82: </a>        <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Irecv_enqueue(link-&gt;rootbuf[PETSCSF_REMOTE][rootmtype_mpi] + disp, cnt, unit, bas-&gt;iranks[i], link-&gt;tag, stream_comm, link-&gt;rootreqs[direction][rootmtype_mpi][rootdirect_mpi] + j));
<a name="line83"> 83: </a>      }
<a name="line84"> 84: </a>    } <font color="#4169E1">else</font> { // PETSCSF_../../../../../..2LEAF
<a name="line85"> 85: </a>      <font color="#4169E1">for</font> (i = ndrootranks, j = 0; i &lt; nrootranks; i++, j++) {
<a name="line86"> 86: </a>        disp = (rootoffset[i] - rootoffset[ndrootranks]) * link-&gt;unitbytes;
<a name="line87"> 87: </a>        cnt  = rootoffset[i + 1] - rootoffset[i];
<a name="line88"> 88: </a>        // no need to sync the gpu stream!
<a name="line89"> 89: </a>        <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Isend_enqueue(link-&gt;rootbuf[PETSCSF_REMOTE][rootmtype_mpi] + disp, cnt, unit, bas-&gt;iranks[i], link-&gt;tag, stream_comm, link-&gt;rootreqs[direction][rootmtype_mpi][rootdirect_mpi] + j));
<a name="line90"> 90: </a>      }
<a name="line91"> 91: </a>    }
<a name="line92"> 92: </a>  }

<a name="line94"> 94: </a>  <font color="#4169E1">if</font> (sf-&gt;leafbuflen[PETSCSF_REMOTE]) {
<a name="line95"> 95: </a>    <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFGetLeafInfo_Basic(sf, &amp;nleafranks, &amp;ndleafranks, NULL, &amp;leafoffset, NULL, NULL));
<a name="line96"> 96: </a>    <font color="#4169E1">if</font> (direction == PETSCSF_LEAF2../../../../../..) {
<a name="line97"> 97: </a>      <font color="#4169E1">for</font> (i = ndleafranks, j = 0; i &lt; nleafranks; i++, j++) {
<a name="line98"> 98: </a>        disp = (leafoffset[i] - leafoffset[ndleafranks]) * link-&gt;unitbytes;
<a name="line99"> 99: </a>        cnt  = leafoffset[i + 1] - leafoffset[i];
<a name="line100">100: </a>        // no need to sync the gpu stream!
<a name="line101">101: </a>        <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Isend_enqueue(link-&gt;leafbuf[PETSCSF_REMOTE][leafmtype_mpi] + disp, cnt, unit, sf-&gt;ranks[i], link-&gt;tag, stream_comm, link-&gt;leafreqs[direction][leafmtype_mpi][leafdirect_mpi] + j));
<a name="line102">102: </a>      }
<a name="line103">103: </a>    } <font color="#4169E1">else</font> { // PETSCSF_../../../../../..2LEAF
<a name="line104">104: </a>      <font color="#4169E1">for</font> (i = ndleafranks, j = 0; i &lt; nleafranks; i++, j++) {
<a name="line105">105: </a>        disp = (leafoffset[i] - leafoffset[ndleafranks]) * link-&gt;unitbytes;
<a name="line106">106: </a>        cnt  = leafoffset[i + 1] - leafoffset[i];
<a name="line107">107: </a>        <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Irecv_enqueue(link-&gt;leafbuf[PETSCSF_REMOTE][leafmtype_mpi] + disp, cnt, unit, sf-&gt;ranks[i], link-&gt;tag, stream_comm, link-&gt;leafreqs[direction][leafmtype_mpi][leafdirect_mpi] + j));
<a name="line108">108: </a>      }
<a name="line109">109: </a>    }
<a name="line110">110: </a>  }
<a name="line111">111: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionReturn.html">PetscFunctionReturn</a>(<a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>);
<a name="line112">112: </a>}

<a name="line114">114: </a><strong><font color="#4169E1"><a name="PetscSFLinkWaitEnqueue_MPIX_Stream"></a>static <a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFLinkWaitEnqueue_MPIX_Stream(<a href="../../../../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> sf, PetscSFLink link, PetscSFDirection direction)</font></strong>
<a name="line115">115: </a>{
<a name="line116">116: </a>  PetscSF_Basic     *bas           = (PetscSF_Basic *)sf-&gt;data;
<a name="line117">117: </a>  const <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a> rootmtype_mpi = link-&gt;rootmtype_mpi, leafmtype_mpi = link-&gt;leafmtype_mpi;
<a name="line118">118: </a>  const <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>     rootdirect_mpi = link-&gt;rootdirect_mpi, leafdirect_mpi = link-&gt;leafdirect_mpi;

<a name="line120">120: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionBegin.html">PetscFunctionBegin</a>;
<a name="line121">121: </a>  <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Waitall_enqueue(bas-&gt;nrootreqs, link-&gt;rootreqs[direction][rootmtype_mpi][rootdirect_mpi], MPI_STATUSES_IGNORE));
<a name="line122">122: </a>  <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(MPIX_Waitall_enqueue(sf-&gt;nleafreqs, link-&gt;leafreqs[direction][leafmtype_mpi][leafdirect_mpi], MPI_STATUSES_IGNORE));
<a name="line123">123: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionReturn.html">PetscFunctionReturn</a>(<a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>);
<a name="line124">124: </a>}
<a name="line125">125: </a><font color="#A020F0">#endif</font>

<a name="line127">127: </a><font color="#B22222">/*</font>
<a name="line128">128: </a><font color="#B22222">   The routine Creates a communication link for the given operation. It first looks up its link cache. If</font>
<a name="line129">129: </a><font color="#B22222">   there is a free &amp; suitable one, it uses it. Otherwise it creates a new one.</font>

<a name="line131">131: </a><font color="#B22222">   A link contains buffers and MPI requests for send/recv. It also contains pack/unpack routines to pack/unpack</font>
<a name="line132">132: </a><font color="#B22222">   root/leafdata to/from these buffers. Buffers are allocated at our discretion. When we find root/leafata</font>
<a name="line133">133: </a><font color="#B22222">   can be directly passed to MPI, we won't allocate them. Even we allocate buffers, we only allocate</font>
<a name="line134">134: </a><font color="#B22222">   those that are needed by the given `sfop` and `op`, in other words, we do lazy memory-allocation.</font>

<a name="line136">136: </a><font color="#B22222">   The routine also allocates buffers on CPU when one does not use gpu-aware MPI but data is on GPU.</font>

<a name="line138">138: </a><font color="#B22222">   In SFBasic, MPI requests are persistent. They are init'ed until we try to get requests from a link.</font>

<a name="line140">140: </a><font color="#B22222">   The routine is shared by SFBasic and SFNeighbor based on the fact they all deal with sparse graphs and</font>
<a name="line141">141: </a><font color="#B22222">   need pack/unpack data.</font>
<a name="line142">142: </a><font color="#B22222">*/</font>
<a name="line143">143: </a><strong><font color="#4169E1"><a name="PetscSFLinkCreate_MPI"></a><a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFLinkCreate_MPI(<a href="../../../../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> sf, MPI_Datatype unit, <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a> xrootmtype, const void *rootdata, <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a> xleafmtype, const void *leafdata, MPI_Op op, PetscSFOperation sfop, PetscSFLink *mylink)</font></strong>
<a name="line144">144: </a>{
<a name="line145">145: </a>  PetscSF_Basic   *bas = (PetscSF_Basic *)sf-&gt;data;
<a name="line146">146: </a>  <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>         i, j, k, nrootreqs, nleafreqs, nreqs;
<a name="line147">147: </a>  PetscSFLink     *p, link;
<a name="line148">148: </a>  PetscSFDirection direction;
<a name="line149">149: </a>  MPI_Request     *reqs = NULL;
<a name="line150">150: </a>  <a href="../../../../../../manualpages/Sys/PetscBool.html">PetscBool</a>        match, rootdirect[2], leafdirect[2];
<a name="line151">151: </a>  <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>     rootmtype = PetscMemTypeHost(xrootmtype) ? <a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a> : <a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_DEVICE</a>; <font color="#B22222">/* Convert to 0/1 as we will use it in subscript */</font>
<a name="line152">152: </a>  <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>     leafmtype = PetscMemTypeHost(xleafmtype) ? <a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a> : <a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_DEVICE</a>;
<a name="line153">153: </a>  <a href="../../../../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>     rootmtype_mpi, leafmtype_mpi;   <font color="#B22222">/* mtypes seen by MPI */</font>
<a name="line154">154: </a>  <a href="../../../../../../manualpages/Sys/PetscInt.html">PetscInt</a>         rootdirect_mpi, leafdirect_mpi; <font color="#B22222">/* root/leafdirect seen by MPI*/</font>

<a name="line156">156: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionBegin.html">PetscFunctionBegin</a>;

<a name="line158">158: </a>  <font color="#B22222">/* Can we directly use root/leafdirect with the given sf, sfop and op? */</font>
<a name="line159">159: </a>  <font color="#4169E1">for</font> (i = PETSCSF_LOCAL; i &lt;= PETSCSF_REMOTE; i++) {
<a name="line160">160: </a>    <font color="#4169E1">if</font> (sfop == PETSCSF_BCAST) {
<a name="line161">161: </a>      rootdirect[i] = bas-&gt;rootcontig[i];                                                  <font color="#B22222">/* Pack roots */</font>
<a name="line162">162: </a>      leafdirect[i] = (sf-&gt;leafcontig[i] &amp;&amp; op == MPI_REPLACE) ? <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_TRUE</a> : <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>; <font color="#B22222">/* Unpack leaves */</font>
<a name="line163">163: </a>    } <font color="#4169E1">else</font> <font color="#4169E1">if</font> (sfop == PETSCSF_REDUCE) {
<a name="line164">164: </a>      leafdirect[i] = sf-&gt;leafcontig[i];                                                    <font color="#B22222">/* Pack leaves */</font>
<a name="line165">165: </a>      rootdirect[i] = (bas-&gt;rootcontig[i] &amp;&amp; op == MPI_REPLACE) ? <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_TRUE</a> : <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>; <font color="#B22222">/* Unpack roots */</font>
<a name="line166">166: </a>    } <font color="#4169E1">else</font> {                                                                                <font color="#B22222">/* PETSCSF_FETCH */</font>
<a name="line167">167: </a>      rootdirect[i] = <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>;                                                          <font color="#B22222">/* FETCH always need a separate rootbuf */</font>
<a name="line168">168: </a>      leafdirect[i] = <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>;                                                          <font color="#B22222">/* We also force allocating a separate leafbuf so that leafdata and leafupdate can share mpi requests */</font>
<a name="line169">169: </a>    }
<a name="line170">170: </a>  }

<a name="line172">172: </a>  <font color="#4169E1">if</font> (sf-&gt;use_gpu_aware_mpi) {
<a name="line173">173: </a>    rootmtype_mpi = rootmtype;
<a name="line174">174: </a>    leafmtype_mpi = leafmtype;
<a name="line175">175: </a>  } <font color="#4169E1">else</font> {
<a name="line176">176: </a>    rootmtype_mpi = leafmtype_mpi = <a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>;
<a name="line177">177: </a>  }
<a name="line178">178: </a>  <font color="#B22222">/* Will root/leafdata be directly accessed by MPI?  Without use_gpu_aware_mpi, device data is buffered on host and then passed to MPI */</font>
<a name="line179">179: </a>  rootdirect_mpi = rootdirect[PETSCSF_REMOTE] &amp;&amp; (rootmtype_mpi == rootmtype) ? 1 : 0;
<a name="line180">180: </a>  leafdirect_mpi = leafdirect[PETSCSF_REMOTE] &amp;&amp; (leafmtype_mpi == leafmtype) ? 1 : 0;

<a name="line182">182: </a>  direction = (sfop == PETSCSF_BCAST) ? PETSCSF_../../../../../..2LEAF : PETSCSF_LEAF2../../../../../..;
<a name="line183">183: </a>  nrootreqs = bas-&gt;nrootreqs;
<a name="line184">184: </a>  nleafreqs = sf-&gt;nleafreqs;

<a name="line186">186: </a>  <font color="#B22222">/* Look for free links in cache */</font>
<a name="line187">187: </a>  <font color="#4169E1">for</font> (p = &amp;bas-&gt;avail; (link = *p); p = &amp;link-&gt;next) {
<a name="line188">188: </a>    <font color="#4169E1">if</font> (!link-&gt;use_nvshmem) { <font color="#B22222">/* Only check with MPI links */</font>
<a name="line189">189: </a>      <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(MPIPetsc_Type_compare(unit, link-&gt;unit, &amp;match));
<a name="line190">190: </a>      <font color="#4169E1">if</font> (match) {
<a name="line191">191: </a>        <font color="#B22222">/* If root/leafdata will be directly passed to MPI, test if the data used to initialized the MPI requests matches with the current.</font>
<a name="line192">192: </a><font color="#B22222">           If not, free old requests. New requests will be lazily init'ed until one calls PetscSFLinkGetMPIBuffersAndRequests().</font>
<a name="line193">193: </a><font color="#B22222">        */</font>
<a name="line194">194: </a>        <font color="#4169E1">if</font> (rootdirect_mpi &amp;&amp; sf-&gt;persistent &amp;&amp; link-&gt;rootreqsinited[direction][rootmtype][1] &amp;&amp; link-&gt;rootdatadirect[direction][rootmtype] != rootdata) {
<a name="line195">195: </a>          reqs = link-&gt;rootreqs[direction][rootmtype][1]; <font color="#B22222">/* Here, rootmtype = rootmtype_mpi */</font>
<a name="line196">196: </a>          <font color="#4169E1">for</font> (i = 0; i &lt; nrootreqs; i++) {
<a name="line197">197: </a>            <font color="#4169E1">if</font> (reqs[i] != MPI_REQUEST_NULL) <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Request_free.html#MPI_Request_free">MPI_Request_free</a>(&amp;reqs[i]));
<a name="line198">198: </a>          }
<a name="line199">199: </a>          link-&gt;rootreqsinited[direction][rootmtype][1] = <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>;
<a name="line200">200: </a>        }
<a name="line201">201: </a>        <font color="#4169E1">if</font> (leafdirect_mpi &amp;&amp; sf-&gt;persistent &amp;&amp; link-&gt;leafreqsinited[direction][leafmtype][1] &amp;&amp; link-&gt;leafdatadirect[direction][leafmtype] != leafdata) {
<a name="line202">202: </a>          reqs = link-&gt;leafreqs[direction][leafmtype][1];
<a name="line203">203: </a>          <font color="#4169E1">for</font> (i = 0; i &lt; nleafreqs; i++) {
<a name="line204">204: </a>            <font color="#4169E1">if</font> (reqs[i] != MPI_REQUEST_NULL) <a href="../../../../../../manualpages/Sys/PetscCallMPI.html">PetscCallMPI</a>(<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Request_free.html#MPI_Request_free">MPI_Request_free</a>(&amp;reqs[i]));
<a name="line205">205: </a>          }
<a name="line206">206: </a>          link-&gt;leafreqsinited[direction][leafmtype][1] = <a href="../../../../../../manualpages/Sys/PetscBool.html">PETSC_FALSE</a>;
<a name="line207">207: </a>        }
<a name="line208">208: </a>        *p = link-&gt;next; <font color="#B22222">/* Remove from available list */</font>
<a name="line209">209: </a>        <font color="#4169E1">goto</font> found;
<a name="line210">210: </a>      }
<a name="line211">211: </a>    }
<a name="line212">212: </a>  }

<a name="line214">214: </a>  <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(<a href="../../../../../../manualpages/Sys/PetscNew.html">PetscNew</a>(&amp;link));
<a name="line215">215: </a>  <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkSetUp_Host(sf, link, unit));
<a name="line216">216: </a>  <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(<a href="../../../../../../manualpages/Sys/PetscCommGetNewTag.html">PetscCommGetNewTag</a>(<a href="../../../../../../manualpages/Sys/PetscObjectComm.html">PetscObjectComm</a>((<a href="../../../../../../manualpages/Sys/PetscObject.html">PetscObject</a>)sf), &amp;link-&gt;tag)); <font color="#B22222">/* One tag per link */</font>

<a name="line218">218: </a>  nreqs = (nrootreqs + nleafreqs) * 8;
<a name="line219">219: </a>  <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(<a href="../../../../../../manualpages/Sys/PetscMalloc1.html">PetscMalloc1</a>(nreqs, &amp;link-&gt;reqs));
<a name="line220">220: </a>  <font color="#4169E1">for</font> (i = 0; i &lt; nreqs; i++) link-&gt;reqs[i] = MPI_REQUEST_NULL; <font color="#B22222">/* Initialized to NULL so that we know which need to be freed in Destroy */</font>

<a name="line222">222: </a>  <font color="#4169E1">for</font> (i = 0; i &lt; 2; i++) {     <font color="#B22222">/* Two communication directions */</font>
<a name="line223">223: </a>    <font color="#4169E1">for</font> (j = 0; j &lt; 2; j++) {   <font color="#B22222">/* Two memory types */</font>
<a name="line224">224: </a>      <font color="#4169E1">for</font> (k = 0; k &lt; 2; k++) { <font color="#B22222">/* root/leafdirect 0 or 1 */</font>
<a name="line225">225: </a>        link-&gt;rootreqs[i][j][k] = link-&gt;reqs + nrootreqs * (4 * i + 2 * j + k);
<a name="line226">226: </a>        link-&gt;leafreqs[i][j][k] = link-&gt;reqs + nrootreqs * 8 + nleafreqs * (4 * i + 2 * j + k);
<a name="line227">227: </a>      }
<a name="line228">228: </a>    }
<a name="line229">229: </a>  }
<a name="line230">230: </a>  link-&gt;StartCommunication  = PetscSFLinkStartRequests_MPI;
<a name="line231">231: </a>  link-&gt;FinishCommunication = PetscSFLinkWaitRequests_MPI;
<a name="line232">232: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPIX_STREAM)</font>
<a name="line233">233: </a>  <font color="#4169E1">if</font> (sf-&gt;use_stream_aware_mpi &amp;&amp; (PetscMemTypeDevice(rootmtype_mpi) || PetscMemTypeDevice(leafmtype_mpi))) {
<a name="line234">234: </a>    link-&gt;StartCommunication  = PetscSFLinkStartEnqueue_MPIX_Stream;
<a name="line235">235: </a>    link-&gt;FinishCommunication = PetscSFLinkWaitEnqueue_MPIX_Stream;
<a name="line236">236: </a>  }
<a name="line237">237: </a><font color="#A020F0">#endif</font>

<a name="line239">239: </a><strong><font color="#FF0000">found:</font></strong>

<a name="line241">241: </a><font color="#A020F0">#if defined(PETSC_HAVE_DEVICE)</font>
<a name="line242">242: </a>  <font color="#4169E1">if</font> ((PetscMemTypeDevice(xrootmtype) || PetscMemTypeDevice(xleafmtype)) &amp;&amp; !link-&gt;deviceinited) {
<a name="line243">243: </a><font color="#A020F0">  #if defined(PETSC_HAVE_CUDA)</font>
<a name="line244">244: </a>    <font color="#4169E1">if</font> (sf-&gt;backend == PETSCSF_BACKEND_CUDA) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkSetUp_CUDA(sf, link, unit)); <font color="#B22222">/* Setup streams etc */</font>
<a name="line245">245: </a><font color="#A020F0">  #endif</font>
<a name="line246">246: </a><font color="#A020F0">  #if defined(PETSC_HAVE_HIP)</font>
<a name="line247">247: </a>    <font color="#4169E1">if</font> (sf-&gt;backend == PETSCSF_BACKEND_HIP) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkSetUp_HIP(sf, link, unit)); <font color="#B22222">/* Setup streams etc */</font>
<a name="line248">248: </a><font color="#A020F0">  #endif</font>
<a name="line249">249: </a><font color="#A020F0">  #if defined(PETSC_HAVE_KOKKOS)</font>
<a name="line250">250: </a>    <font color="#4169E1">if</font> (sf-&gt;backend == PETSCSF_BACKEND_KOKKOS) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFLinkSetUp_Kokkos(sf, link, unit));
<a name="line251">251: </a><font color="#A020F0">  #endif</font>
<a name="line252">252: </a>  }
<a name="line253">253: </a><font color="#A020F0">#endif</font>

<a name="line255">255: </a>  <font color="#B22222">/* Allocate buffers along root/leafdata */</font>
<a name="line256">256: </a>  <font color="#4169E1">for</font> (i = PETSCSF_LOCAL; i &lt;= PETSCSF_REMOTE; i++) {
<a name="line257">257: </a>    <font color="#B22222">/* For local communication, buffers are only needed when roots and leaves have different mtypes */</font>
<a name="line258">258: </a>    <font color="#4169E1">if</font> (i == PETSCSF_LOCAL &amp;&amp; rootmtype == leafmtype) <font color="#4169E1">continue</font>;
<a name="line259">259: </a>    <font color="#4169E1">if</font> (bas-&gt;rootbuflen[i]) {
<a name="line260">260: </a>      <font color="#4169E1">if</font> (rootdirect[i]) { <font color="#B22222">/* Aha, we disguise rootdata as rootbuf */</font>
<a name="line261">261: </a>        link-&gt;rootbuf[i][rootmtype] = (char *)rootdata + bas-&gt;rootstart[i] * link-&gt;unitbytes;
<a name="line262">262: </a>      } <font color="#4169E1">else</font> { <font color="#B22222">/* Have to have a separate rootbuf */</font>
<a name="line263">263: </a>        <font color="#4169E1">if</font> (!link-&gt;rootbuf_alloc[i][rootmtype]) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFMalloc(sf, rootmtype, bas-&gt;rootbuflen[i] * link-&gt;unitbytes, (void **)&amp;link-&gt;rootbuf_alloc[i][rootmtype]));
<a name="line264">264: </a>        link-&gt;rootbuf[i][rootmtype] = link-&gt;rootbuf_alloc[i][rootmtype];
<a name="line265">265: </a>      }
<a name="line266">266: </a>    }

<a name="line268">268: </a>    <font color="#4169E1">if</font> (sf-&gt;leafbuflen[i]) {
<a name="line269">269: </a>      <font color="#4169E1">if</font> (leafdirect[i]) {
<a name="line270">270: </a>        link-&gt;leafbuf[i][leafmtype] = (char *)leafdata + sf-&gt;leafstart[i] * link-&gt;unitbytes;
<a name="line271">271: </a>      } <font color="#4169E1">else</font> {
<a name="line272">272: </a>        <font color="#4169E1">if</font> (!link-&gt;leafbuf_alloc[i][leafmtype]) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(PetscSFMalloc(sf, leafmtype, sf-&gt;leafbuflen[i] * link-&gt;unitbytes, (void **)&amp;link-&gt;leafbuf_alloc[i][leafmtype]));
<a name="line273">273: </a>        link-&gt;leafbuf[i][leafmtype] = link-&gt;leafbuf_alloc[i][leafmtype];
<a name="line274">274: </a>      }
<a name="line275">275: </a>    }
<a name="line276">276: </a>  }

<a name="line278">278: </a><font color="#A020F0">#if defined(PETSC_HAVE_DEVICE)</font>
<a name="line279">279: </a>  <font color="#B22222">/* Allocate buffers on host for buffering data on device in cast not use_gpu_aware_mpi */</font>
<a name="line280">280: </a>  <font color="#4169E1">if</font> (PetscMemTypeDevice(rootmtype) &amp;&amp; PetscMemTypeHost(rootmtype_mpi)) {
<a name="line281">281: </a>    <font color="#4169E1">if</font> (!link-&gt;rootbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>]) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(<a href="../../../../../../manualpages/Sys/PetscMalloc.html">PetscMalloc</a>(bas-&gt;rootbuflen[PETSCSF_REMOTE] * link-&gt;unitbytes, &amp;link-&gt;rootbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>]));
<a name="line282">282: </a>    link-&gt;rootbuf[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>] = link-&gt;rootbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>];
<a name="line283">283: </a>  }
<a name="line284">284: </a>  <font color="#4169E1">if</font> (PetscMemTypeDevice(leafmtype) &amp;&amp; PetscMemTypeHost(leafmtype_mpi)) {
<a name="line285">285: </a>    <font color="#4169E1">if</font> (!link-&gt;leafbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>]) <a href="../../../../../../manualpages/Sys/PetscCall.html">PetscCall</a>(<a href="../../../../../../manualpages/Sys/PetscMalloc.html">PetscMalloc</a>(sf-&gt;leafbuflen[PETSCSF_REMOTE] * link-&gt;unitbytes, &amp;link-&gt;leafbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>]));
<a name="line286">286: </a>    link-&gt;leafbuf[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>] = link-&gt;leafbuf_alloc[PETSCSF_REMOTE][<a href="../../../../../../manualpages/Sys/PetscMemType.html">PETSC_MEMTYPE_HOST</a>];
<a name="line287">287: </a>  }
<a name="line288">288: </a><font color="#A020F0">#endif</font>

<a name="line290">290: </a>  <font color="#B22222">/* Set `current` state of the link. They may change between different SF invocations with the same link */</font>
<a name="line291">291: </a>  <font color="#4169E1">if</font> (sf-&gt;persistent) { <font color="#B22222">/* If data is directly passed to MPI and inits MPI requests, record the data for comparison on future invocations */</font>
<a name="line292">292: </a>    <font color="#4169E1">if</font> (rootdirect_mpi) link-&gt;rootdatadirect[direction][rootmtype] = rootdata;
<a name="line293">293: </a>    <font color="#4169E1">if</font> (leafdirect_mpi) link-&gt;leafdatadirect[direction][leafmtype] = leafdata;
<a name="line294">294: </a>  }

<a name="line296">296: </a>  link-&gt;rootdata = rootdata; <font color="#B22222">/* root/leafdata are keys to look up links in PetscSFXxxEnd */</font>
<a name="line297">297: </a>  link-&gt;leafdata = leafdata;
<a name="line298">298: </a>  <font color="#4169E1">for</font> (i = PETSCSF_LOCAL; i &lt;= PETSCSF_REMOTE; i++) {
<a name="line299">299: </a>    link-&gt;rootdirect[i] = rootdirect[i];
<a name="line300">300: </a>    link-&gt;leafdirect[i] = leafdirect[i];
<a name="line301">301: </a>  }
<a name="line302">302: </a>  link-&gt;rootdirect_mpi = rootdirect_mpi;
<a name="line303">303: </a>  link-&gt;leafdirect_mpi = leafdirect_mpi;
<a name="line304">304: </a>  link-&gt;rootmtype      = rootmtype;
<a name="line305">305: </a>  link-&gt;leafmtype      = leafmtype;
<a name="line306">306: </a>  link-&gt;rootmtype_mpi  = rootmtype_mpi;
<a name="line307">307: </a>  link-&gt;leafmtype_mpi  = leafmtype_mpi;

<a name="line309">309: </a>  link-&gt;next = bas-&gt;inuse;
<a name="line310">310: </a>  bas-&gt;inuse = link;
<a name="line311">311: </a>  *mylink    = link;
<a name="line312">312: </a>  <a href="../../../../../../manualpages/Sys/PetscFunctionReturn.html">PetscFunctionReturn</a>(<a href="../../../../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>);
<a name="line313">313: </a>}
</pre>
</body>

</html>
