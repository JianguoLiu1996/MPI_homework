<center><a href="https://gitlab.com/petsc/petsc/-/blob/a093114c2e7b4ba1f47e6ac8a72c57724150f16d/include/petsc/private/sfimpl.h">Actual source code: sfimpl.h</a></center><br>

<html>
<head>
<title></title>
<meta name="generator" content="c2html 0.9.4">
<meta name="date" content="2023-04-30T14:19:23+00:00">
</head>

<body bgcolor="#FFFFFF">
<pre width="80">
<a name="line1">  1: </a><font color="#A020F0">#ifndef SFIMPL_H</font>
<a name="line2">  2: </a><strong><font color="#228B22">#define SFIMPL_H</font></strong>

<a name="line4">  4: </a>#include <A href="../../petscvec.h.html">&lt;petscvec.h&gt;</A>
<a name="line5">  5: </a>#include <A href="../../petscsf.h.html">&lt;petscsf.h&gt;</A>
<a name="line6">  6: </a>#include <A href="deviceimpl.h.html">&lt;petsc/private/deviceimpl.h&gt;</A>
<a name="line7">  7: </a>#include <A href="mpiutils.h.html">&lt;petsc/private/mpiutils.h&gt;</A>
<a name="line8">  8: </a>#include <A href="petscimpl.h.html">&lt;petsc/private/petscimpl.h&gt;</A>

<a name="line10"> 10: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_SetGraph;
<a name="line11"> 11: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_SetUp;
<a name="line12"> 12: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_BcastBegin;
<a name="line13"> 13: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_BcastEnd;
<a name="line14"> 14: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_BcastBegin;
<a name="line15"> 15: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_BcastEnd;
<a name="line16"> 16: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_ReduceBegin;
<a name="line17"> 17: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_ReduceEnd;
<a name="line18"> 18: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_FetchAndOpBegin;
<a name="line19"> 19: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_FetchAndOpEnd;
<a name="line20"> 20: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_EmbedSF;
<a name="line21"> 21: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_DistSect;
<a name="line22"> 22: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_SectSF;
<a name="line23"> 23: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_RemoteOff;
<a name="line24"> 24: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_Pack;
<a name="line25"> 25: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscLogEvent.html">PetscLogEvent</a> PETSCSF_Unpack;

<a name="line27"> 27: </a><font color="#4169E1">typedef</font> <font color="#4169E1">enum</font> {
<a name="line28"> 28: </a>  PETSCSF_../../..2LEAF = 0,
<a name="line29"> 29: </a>  PETSCSF_LEAF2../../..
<a name="line30"> 30: </a>} PetscSFDirection;
<a name="line31"> 31: </a><font color="#4169E1">typedef</font> <font color="#4169E1">enum</font> {
<a name="line32"> 32: </a>  PETSCSF_BCAST = 0,
<a name="line33"> 33: </a>  PETSCSF_REDUCE,
<a name="line34"> 34: </a>  PETSCSF_FETCH
<a name="line35"> 35: </a>} PetscSFOperation;
<a name="line36"> 36: </a><font color="#B22222">/* When doing device-aware MPI, a backend refers to the SF/device interface */</font>
<a name="line37"> 37: </a><font color="#4169E1">typedef</font> <font color="#4169E1">enum</font> {
<a name="line38"> 38: </a>  PETSCSF_BACKEND_INVALID = 0,
<a name="line39"> 39: </a>  PETSCSF_BACKEND_CUDA,
<a name="line40"> 40: </a>  PETSCSF_BACKEND_HIP,
<a name="line41"> 41: </a>  PETSCSF_BACKEND_KOKKOS
<a name="line42"> 42: </a>} PetscSFBackend;

<a name="line44"> 44: </a><font color="#4169E1"><a name="_PetscSFOps"></a>struct _PetscSFOps </font>{
<a name="line45"> 45: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*Reset)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>);
<a name="line46"> 46: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*Destroy)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>);
<a name="line47"> 47: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*SetUp)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>);
<a name="line48"> 48: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*SetFromOptions)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, PetscOptionItems *);
<a name="line49"> 49: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*View)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Viewer/PetscViewer.html">PetscViewer</a>);
<a name="line50"> 50: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*Duplicate)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/PetscSF/PetscSFDuplicateOption.html">PetscSFDuplicateOption</a>, <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>);
<a name="line51"> 51: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*BcastBegin)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, const void *, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *, MPI_Op);
<a name="line52"> 52: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*BcastEnd)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, const void *, void *, MPI_Op);
<a name="line53"> 53: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*ReduceBegin)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, const void *, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *, MPI_Op);
<a name="line54"> 54: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*ReduceEnd)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, const void *, void *, MPI_Op);
<a name="line55"> 55: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*FetchAndOpBegin)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, const void *, void *, MPI_Op);
<a name="line56"> 56: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*FetchAndOpEnd)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, void *, const void *, void *, MPI_Op);
<a name="line57"> 57: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*BcastToZero)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, const void *, <a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *); <font color="#B22222">/* For internal use only */</font>
<a name="line58"> 58: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*GetRootRanks)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **);
<a name="line59"> 59: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*GetLeafRanks)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **);
<a name="line60"> 60: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*CreateLocalSF)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> *);
<a name="line61"> 61: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*GetGraph)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/PetscSF/PetscSFNode.html">PetscSFNode</a> **);
<a name="line62"> 62: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*CreateEmbeddedRootSF)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> *);
<a name="line63"> 63: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*CreateEmbeddedLeafSF)(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> *);

<a name="line65"> 65: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*Malloc)(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, size_t, void **);
<a name="line66"> 66: </a>  <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> (*Free)(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *);
<a name="line67"> 67: </a>};

<a name="line69"> 69: </a><font color="#4169E1">typedef struct _n_PetscSFPackOpt *PetscSFPackOpt;</font>

<a name="line71"> 71: </a><font color="#4169E1"><a name="_p_PetscSF"></a>struct _p_PetscSF </font>{
<a name="line72"> 72: </a>  PETSCHEADER(<font color="#4169E1">struct _PetscSFOps</font>);
<a name="line73"> 73: </a>  <font color="#4169E1">struct</font> {                                  <font color="#B22222">/* Fields needed to implement <a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a> behavior */</font>
<a name="line74"> 74: </a>    <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>           from_n, to_n;        <font color="#B22222">/* Recorded local sizes of the input from/to vectors in <a href="../../../manualpages/PetscSF/VecScatterCreate.html">VecScatterCreate</a>(). Used subsequently for error checking. */</font>
<a name="line75"> 75: </a>    <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>          beginandendtogether; <font color="#B22222">/* Indicates that the scatter begin and end  function are called together, <a href="../../../manualpages/PetscSF/VecScatterEnd.html">VecScatterEnd</a>() is then treated as a nop */</font>
<a name="line76"> 76: </a>    const <a href="../../../manualpages/Sys/PetscScalar.html">PetscScalar</a> *xdata;               <font color="#B22222">/* Vector data to read from */</font>
<a name="line77"> 77: </a>    <a href="../../../manualpages/Sys/PetscScalar.html">PetscScalar</a>       *ydata;               <font color="#B22222">/* Vector data to write to. The two pointers are recorded in <a href="../../../manualpages/PetscSF/VecScatterBegin.html">VecScatterBegin</a>. Memory is not managed by SF. */</font>
<a name="line78"> 78: </a>    <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>            lsf;                 <font color="#B22222">/* The local part of the scatter, used in SCATTER_LOCAL. Built on demand. */</font>
<a name="line79"> 79: </a>    <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>           bs;                  <font color="#B22222">/* Block size, determined by <a href="../../../manualpages/IS/IS.html">IS</a> passed to <a href="../../../manualpages/PetscSF/VecScatterCreate.html">VecScatterCreate</a> */</font>
<a name="line80"> 80: </a>    MPI_Datatype       unit;                <font color="#B22222">/* one unit = bs PetscScalars */</font>
<a name="line81"> 81: </a>    <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>          logging;             <font color="#B22222">/* Indicate if vscat log events are happening. If yes, avoid duplicated SF logging to have clear -log_view */</font>
<a name="line82"> 82: </a>  } vscat;

<a name="line84"> 84: </a>  <font color="#B22222">/* Fields for generic <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> functionality */</font>
<a name="line85"> 85: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>     nroots;  <font color="#B22222">/* Number of root vertices on current process (candidates for incoming edges) */</font>
<a name="line86"> 86: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>     nleaves; <font color="#B22222">/* Number of leaf vertices on current process (this process specifies a root for each leaf) */</font>
<a name="line87"> 87: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *mine;    <font color="#B22222">/* Location of leaves in leafdata arrays provided to the communication routines */</font>
<a name="line88"> 88: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *mine_alloc;
<a name="line89"> 89: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>     minleaf, maxleaf;
<a name="line90"> 90: </a>  <a href="../../../manualpages/PetscSF/PetscSFNode.html">PetscSFNode</a> *remote; <font color="#B22222">/* Remote references to roots for each local leaf */</font>
<a name="line91"> 91: </a>  <a href="../../../manualpages/PetscSF/PetscSFNode.html">PetscSFNode</a> *remote_alloc;
<a name="line92"> 92: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>     nranks;     <font color="#B22222">/* Number of ranks owning roots connected to my leaves */</font>
<a name="line93"> 93: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>     ndranks;    <font color="#B22222">/* Number of ranks in distinguished group holding roots connected to my leaves */</font>
<a name="line94"> 94: </a>  <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> *ranks;      <font color="#B22222">/* List of ranks referenced by "remote" */</font>
<a name="line95"> 95: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *roffset;    <font color="#B22222">/* Array of length nranks+1, offset in rmine/rremote for each rank */</font>
<a name="line96"> 96: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *rmine;      <font color="#B22222">/* Concatenated array holding local indices referencing each remote rank */</font>
<a name="line97"> 97: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *rmine_d[2]; <font color="#B22222">/* A copy of rmine[local/remote] in device memory if needed */</font>

<a name="line99"> 99: </a>  <font color="#B22222">/* Some results useful in packing by analyzing rmine[] */</font>
<a name="line100">100: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>       leafbuflen[2];    <font color="#B22222">/* Length (in unit) of leaf buffers, in layout of [PETSCSF_LOCAL/REMOTE] */</font>
<a name="line101">101: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      leafcontig[2];    <font color="#B22222">/* True means indices in rmine[self part] or rmine[remote part] are contiguous, and they start from ... */</font>
<a name="line102">102: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>       leafstart[2];     <font color="#B22222">/* ... leafstart[0] and leafstart[1] respectively */</font>
<a name="line103">103: </a>  PetscSFPackOpt leafpackopt[2];   <font color="#B22222">/* Optimization plans to (un)pack leaves connected to remote roots, based on index patterns in rmine[]. NULL for no optimization */</font>
<a name="line104">104: </a>  PetscSFPackOpt leafpackopt_d[2]; <font color="#B22222">/* Copy of leafpackopt_d[] on device if needed */</font>
<a name="line105">105: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      leafdups[2];      <font color="#B22222">/* Indices in rmine[] for self(0)/remote(1) communication have dups respectively? TRUE implies theads working on them in parallel may have data race. */</font>

<a name="line107">107: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>       nleafreqs;            <font color="#B22222">/* Number of MPI requests for leaves */</font>
<a name="line108">108: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>      *rremote;              <font color="#B22222">/* Concatenated array holding remote indices referenced for each remote rank */</font>
<a name="line109">109: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      degreeknown;          <font color="#B22222">/* The degree is currently known, do not have to recompute */</font>
<a name="line110">110: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>      *degree;               <font color="#B22222">/* Degree of each of my root vertices */</font>
<a name="line111">111: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>      *degreetmp;            <font color="#B22222">/* Temporary local array for computing degree */</font>
<a name="line112">112: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      rankorder;            <font color="#B22222">/* Sort ranks for gather and scatter operations */</font>
<a name="line113">113: </a>  MPI_Group      ingroup;              <font color="#B22222">/* Group of processes connected to my roots */</font>
<a name="line114">114: </a>  MPI_Group      outgroup;             <font color="#B22222">/* Group of processes connected to my leaves */</font>
<a name="line115">115: </a>  <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>        multi;                <font color="#B22222">/* Internal graph used to implement gather and scatter operations */</font>
<a name="line116">116: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      graphset;             <font color="#B22222">/* Flag indicating that the graph has been set, required before calling communication routines */</font>
<a name="line117">117: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      setupcalled;          <font color="#B22222">/* Type and communication structures have been set up */</font>
<a name="line118">118: </a>  <a href="../../../manualpages/PetscSF/PetscSFPattern.html">PetscSFPattern</a> pattern;              <font color="#B22222">/* Pattern of the graph */</font>
<a name="line119">119: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      persistent;           <font color="#B22222">/* Does this SF use MPI persistent requests for communication */</font>
<a name="line120">120: </a>  <a href="../../../manualpages/IS/PetscLayout.html">PetscLayout</a>    map;                  <font color="#B22222">/* Layout of leaves over all processes when building a patterned graph */</font>
<a name="line121">121: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      unknown_input_stream; <font color="#B22222">/* If true, SF does not know which streams root/leafdata is on. Default is false, since we only use petsc default stream */</font>
<a name="line122">122: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      use_gpu_aware_mpi;    <font color="#B22222">/* If true, SF assumes it can pass GPU pointers to MPI */</font>
<a name="line123">123: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      use_stream_aware_mpi; <font color="#B22222">/* If true, SF assumes the underlying MPI is cuda-stream aware and we won't sync streams for send/recv buffers passed to MPI */</font>
<a name="line124">124: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>       maxResidentThreadsPerGPU;
<a name="line125">125: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      allow_multi_leaves;
<a name="line126">126: </a>  PetscSFBackend backend; <font color="#B22222">/* The device backend (if any) SF will use */</font>
<a name="line127">127: </a>  void          *data;    <font color="#B22222">/* Pointer to implementation */</font>

<a name="line129">129: </a><font color="#A020F0">#if defined(PETSC_HAVE_NVSHMEM)</font>
<a name="line130">130: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> use_nvshmem;                 <font color="#B22222">/* TRY to use nvshmem on cuda devices with this SF when possible */</font>
<a name="line131">131: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> use_nvshmem_get;             <font color="#B22222">/* If true, use nvshmem_get based protocol, otherwise, use nvshmem_put based protocol */</font>
<a name="line132">132: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> checked_nvshmem_eligibility; <font color="#B22222">/* Have we checked eligibility of using NVSHMEM on this sf? */</font>
<a name="line133">133: </a>  <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> setup_nvshmem;               <font color="#B22222">/* Have we already set up NVSHMEM related fields below? These fields are built on-demand */</font>
<a name="line134">134: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>  leafbuflen_rmax;             <font color="#B22222">/* max leafbuflen[REMOTE] over comm */</font>
<a name="line135">135: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>  nRemoteRootRanks;            <font color="#B22222">/* nranks - ndranks */</font>
<a name="line136">136: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>  nRemoteRootRanksMax;         <font color="#B22222">/* max nranks-ndranks over comm */</font>

<a name="line138">138: </a>  <font color="#B22222">/* The following two fields look confusing but actually make sense: They are offsets of buffers at the remote side. We're doing one-sided communication! */</font>
<a name="line139">139: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *rootsigdisp; <font color="#B22222">/* [nRemoteRootRanks]. For my i-th remote root rank, I will access its rootsigdisp[i]-th root signal */</font>
<a name="line140">140: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *rootbufdisp; <font color="#B22222">/* [nRemoteRootRanks]. For my i-th remote root rank, I will access its root buf at offset rootbufdisp[i], in &lt;unit&gt; to be set */</font>

<a name="line142">142: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *rootbufdisp_d;
<a name="line143">143: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *rootsigdisp_d; <font color="#B22222">/* Copy of rootsigdisp[] on device */</font>
<a name="line144">144: </a>  <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> *ranks_d;       <font color="#B22222">/* Copy of the remote part of (root) ranks[] on device */</font>
<a name="line145">145: </a>  <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a>    *roffset_d;     <font color="#B22222">/* Copy of the remote part of roffset[] on device */</font>
<a name="line146">146: </a><font color="#A020F0">#endif</font>
<a name="line147">147: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPIX_STREAM)</font>
<a name="line148">148: </a>  MPIX_Stream mpi_stream;
<a name="line149">149: </a>  <a href="../../../manualpages/Sys/MPI_Comm.html">MPI_Comm</a>    stream_comm; <font color="#B22222">/* gpu stream aware MPI communicator */</font>
<a name="line150">150: </a><font color="#A020F0">#endif</font>
<a name="line151">151: </a>};

<a name="line153">153: </a>PETSC_EXTERN <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>      PetscSFRegisterAllCalled;
<a name="line154">154: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../manualpages/PetscSF/PetscSFRegisterAll.html">PetscSFRegisterAll</a>(void)</font></strong>;

<a name="line156">156: </a><strong><font color="#4169E1">PETSC_INTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFCreateLocalSF_Private(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, <a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a> *)</font></strong>;
<a name="line157">157: </a><strong><font color="#4169E1"><a name="PetscSFBcastToZero_Private"></a>PETSC_INTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFBcastToZero_Private(<a href="../../../manualpages/PetscSF/PetscSF.html">PetscSF</a>, MPI_Datatype, const void *, void *)</font></strong> PETSC_ATTRIBUTE_MPI_POINTER_WITH_TYPE(3, 2) PETSC_ATTRIBUTE_MPI_POINTER_WITH_TYPE(4, 2);

<a name="line159">159: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> MPIPetsc_Type_unwrap(MPI_Datatype, MPI_Datatype *, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> *)</font></strong>;
<a name="line160">160: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> MPIPetsc_Type_compare(MPI_Datatype, MPI_Datatype, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a> *)</font></strong>;
<a name="line161">161: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> MPIPetsc_Type_compare_contig(MPI_Datatype, MPI_Datatype, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;

<a name="line163">163: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_NONBLOCKING_COLLECTIVES)</font>
<a name="line164">164: </a><strong><font color="#228B22">  #define MPIU_Ibcast(a, b, c, d, e, req)                <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibcast.html#MPI_Ibcast">MPI_Ibcast</a>(a, b, c, d, e, req)</font></strong>
<a name="line165">165: </a><strong><font color="#228B22">  #define MPIU_Ireduce(a, b, c, d, e, f, g, req)         <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ireduce.html#MPI_Ireduce">MPI_Ireduce</a>(a, b, c, d, e, f, g, req)</font></strong>
<a name="line166">166: </a><strong><font color="#228B22">  #define MPIU_Iscatter(a, b, c, d, e, f, g, h, req)     <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iscatter.html#MPI_Iscatter">MPI_Iscatter</a>(a, b, c, d, e, f, g, h, req)</font></strong>
<a name="line167">167: </a><strong><font color="#228B22">  #define MPIU_Iscatterv(a, b, c, d, e, f, g, h, i, req) <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iscatterv.html#MPI_Iscatterv">MPI_Iscatterv</a>(a, b, c, d, e, f, g, h, i, req)</font></strong>
<a name="line168">168: </a><strong><font color="#228B22">  #define MPIU_Igather(a, b, c, d, e, f, g, h, req)      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Igather.html#MPI_Igather">MPI_Igather</a>(a, b, c, d, e, f, g, h, req)</font></strong>
<a name="line169">169: </a><strong><font color="#228B22">  #define MPIU_Igatherv(a, b, c, d, e, f, g, h, i, req)  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Igatherv.html#MPI_Igatherv">MPI_Igatherv</a>(a, b, c, d, e, f, g, h, i, req)</font></strong>
<a name="line170">170: </a><strong><font color="#228B22">  #define MPIU_Iallgather(a, b, c, d, e, f, g, req)      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iallgather.html#MPI_Iallgather">MPI_Iallgather</a>(a, b, c, d, e, f, g, req)</font></strong>
<a name="line171">171: </a><strong><font color="#228B22">  #define MPIU_Iallgatherv(a, b, c, d, e, f, g, h, req)  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iallgatherv.html#MPI_Iallgatherv">MPI_Iallgatherv</a>(a, b, c, d, e, f, g, h, req)</font></strong>
<a name="line172">172: </a><strong><font color="#228B22">  #define MPIU_Ialltoall(a, b, c, d, e, f, g, req)       <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ialltoall.html#MPI_Ialltoall">MPI_Ialltoall</a>(a, b, c, d, e, f, g, req)</font></strong>
<a name="line173">173: </a><font color="#A020F0">#else</font>
<a name="line174">174: </a>  <font color="#B22222">/* Ignore req, the MPI_Request argument, and use MPI blocking collectives. One should initialize req</font>
<a name="line175">175: </a><font color="#B22222">   to MPI_REQUEST_NULL so that one can do <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Wait.html#MPI_Wait">MPI_Wait</a>(req,status) no matter the call is blocking or not.</font>
<a name="line176">176: </a><font color="#B22222"> */</font>
<a name="line177">177: </a><strong><font color="#228B22">  #define MPIU_Ibcast(a, b, c, d, e, req)                <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</a>(a, b, c, d, e)</font></strong>
<a name="line178">178: </a><strong><font color="#228B22">  #define MPIU_Ireduce(a, b, c, d, e, f, g, req)         <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html#MPI_Reduce">MPI_Reduce</a>(a, b, c, d, e, f, g)</font></strong>
<a name="line179">179: </a><strong><font color="#228B22">  #define MPIU_Iscatter(a, b, c, d, e, f, g, h, req)     <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Scatter.html#MPI_Scatter">MPI_Scatter</a>(a, b, c, d, e, f, g, h)</font></strong>
<a name="line180">180: </a><strong><font color="#228B22">  #define MPIU_Iscatterv(a, b, c, d, e, f, g, h, i, req) <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Scatterv.html#MPI_Scatterv">MPI_Scatterv</a>(a, b, c, d, e, f, g, h, i)</font></strong>
<a name="line181">181: </a><strong><font color="#228B22">  #define MPIU_Igather(a, b, c, d, e, f, g, h, req)      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Gather.html#MPI_Gather">MPI_Gather</a>(a, b, c, d, e, f, g, h)</font></strong>
<a name="line182">182: </a><strong><font color="#228B22">  #define MPIU_Igatherv(a, b, c, d, e, f, g, h, i, req)  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Gatherv.html#MPI_Gatherv">MPI_Gatherv</a>(a, b, c, d, e, f, g, h, i)</font></strong>
<a name="line183">183: </a><strong><font color="#228B22">  #define MPIU_Iallgather(a, b, c, d, e, f, g, req)      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Allgather.html#MPI_Allgather">MPI_Allgather</a>(a, b, c, d, e, f, g)</font></strong>
<a name="line184">184: </a><strong><font color="#228B22">  #define MPIU_Iallgatherv(a, b, c, d, e, f, g, h, req)  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Allgatherv.html#MPI_Allgatherv">MPI_Allgatherv</a>(a, b, c, d, e, f, g, h)</font></strong>
<a name="line185">185: </a><strong><font color="#228B22">  #define MPIU_Ialltoall(a, b, c, d, e, f, g, req)       <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Alltoall.html#MPI_Alltoall">MPI_Alltoall</a>(a, b, c, d, e, f, g)</font></strong>
<a name="line186">186: </a><font color="#A020F0">#endif</font>

<a name="line188">188: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> VecScatterGetRemoteCount_Private(<a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a>, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;
<a name="line189">189: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> VecScatterGetRemote_Private(<a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a>, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;
<a name="line190">190: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> VecScatterGetRemoteOrdered_Private(<a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a>, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;
<a name="line191">191: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> VecScatterRestoreRemote_Private(<a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a>, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;
<a name="line192">192: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> VecScatterRestoreRemoteOrdered_Private(<a href="../../../manualpages/PetscSF/VecScatter.html">VecScatter</a>, <a href="../../../manualpages/Sys/PetscBool.html">PetscBool</a>, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> **, const <a href="../../../manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> **, <a href="../../../manualpages/Sys/PetscInt.html">PetscInt</a> *)</font></strong>;

<a name="line194">194: </a><font color="#A020F0">#if defined(PETSC_HAVE_CUDA)</font>
<a name="line195">195: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFMalloc_CUDA(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, size_t, void **)</font></strong>;
<a name="line196">196: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFFree_CUDA(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *)</font></strong>;
<a name="line197">197: </a><font color="#A020F0">#endif</font>
<a name="line198">198: </a><font color="#A020F0">#if defined(PETSC_HAVE_HIP)</font>
<a name="line199">199: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFMalloc_HIP(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, size_t, void **)</font></strong>;
<a name="line200">200: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFFree_HIP(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *)</font></strong>;
<a name="line201">201: </a><font color="#A020F0">#endif</font>
<a name="line202">202: </a><font color="#A020F0">#if defined(PETSC_HAVE_KOKKOS)</font>
<a name="line203">203: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFMalloc_Kokkos(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, size_t, void **)</font></strong>;
<a name="line204">204: </a><strong><font color="#4169E1">PETSC_EXTERN <a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscSFFree_Kokkos(<a href="../../../manualpages/Sys/PetscMemType.html">PetscMemType</a>, void *)</font></strong>;
<a name="line205">205: </a><font color="#A020F0">#endif</font>

<a name="line207">207: </a><font color="#B22222">/* SF only supports CUDA and Kokkos devices. Even VIENNACL is a device, its device pointers are invisible to SF.</font>
<a name="line208">208: </a><font color="#B22222">   Through <a href="../../../manualpages/Vec/VecGetArray.html">VecGetArray</a>(), we copy data of <a href="../../../manualpages/Vec/VECVIENNACL.html">VECVIENNACL</a> from device to host and pass host pointers to SF.</font>
<a name="line209">209: </a><font color="#B22222"> */</font>
<a name="line210">210: </a><font color="#A020F0">#if defined(PETSC_HAVE_CUDA) || defined(PETSC_HAVE_KOKKOS) || defined(PETSC_HAVE_HIP)</font>
<a name="line211">211: </a><strong><font color="#228B22">  #define PetscSFMalloc(sf, mtype, sz, ptr) ((*(sf)-&gt;ops-&gt;Malloc)(mtype, sz, ptr))</font></strong>
<a name="line212">212: </a>  <font color="#B22222">/* Free memory and set ptr to NULL when succeeded */</font>
<a name="line213">213: </a><strong><font color="#228B22">  #define PetscSFFree(sf, mtype, ptr) ((<a href="../../../manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a>)((ptr) &amp;&amp; ((*(sf)-&gt;ops-&gt;Free)(mtype, ptr) || ((ptr) = NULL, <a href="../../../manualpages/Sys/PetscErrorCode.html">PETSC_SUCCESS</a>))))</font></strong>
<a name="line214">214: </a><font color="#A020F0">#else</font>
<a name="line215">215: </a>  <font color="#B22222">/* If pure host code, do with less indirection */</font>
<a name="line216">216: </a><strong><font color="#228B22">  #define PetscSFMalloc(sf, mtype, sz, ptr) <a href="../../../manualpages/Sys/PetscMalloc.html">PetscMalloc</a>(sz, ptr)</font></strong>
<a name="line217">217: </a><strong><font color="#228B22">  #define PetscSFFree(sf, mtype, ptr)       <a href="../../../manualpages/Sys/PetscFree.html">PetscFree</a>(ptr)</font></strong>
<a name="line218">218: </a><font color="#A020F0">#endif</font>

<a name="line220">220: </a><font color="#A020F0">#endif</font>
</pre>
</body>

</html>
